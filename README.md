# Solar Radiation Forecasting ‚Äì End-to-End ML & Orchestration

Pipeline de **r√©gression horaire** pour pr√©dire la production solaire (irradiation / GHI) √† partir de s√©ries temporelles m√©t√©orologiques, avec :

- entra√Ænement et √©valuation de mod√®les de ML,
- orchestration des t√¢ches avec **Apache Airflow** via **Astro CLI**,
- API de pr√©diction avec **FastAPI**,
- dashboard de visualisation avec **Streamlit**,
- conteneurisation avec **Docker**.

---

## üéØ Objectif

Fournir des pr√©visions fiables de production solaire √† l‚Äô√©chelle horaire pour aider √† :

- optimiser la **gestion √©nerg√©tique**,
- anticiper la production photovolta√Øque,
- tester diff√©rents mod√®les et strat√©gies de d√©ploiement dans un cadre MLOps.

---

## üß± Fonctionnalit√©s principales

- Ingestion et parsing de s√©ries temporelles (horodatage, nettoyage, tri).
- Feature engineering temporel (heures, mois, jour, encodage cyclique, etc.).
- D√©tection et traitement des **outliers**, split temporel **chronologique** train/test.
- Benchmark de mod√®les :
  - `LinearRegression`
  - `RandomForestRegressor`
  - `XGBRegressor`
  - `GradientBoostingRegressor`
- Tuning (GridSearch / RandomSearch) et √©valuation avec :
  - R¬≤
  - MAE
  - MSE
- Orchestration des pipelines via **Airflow** (Astro CLI).
- Exposition d‚Äôun endpoint de pr√©diction via **FastAPI**.
- Dashboard de visualisation / monitoring via **Streamlit**.
- Conteneurisation et ex√©cution via **Docker** / `docker-compose`.

---

## üìä R√©sultats (exemple)

Meilleur mod√®le trouv√© sur la base des exp√©rimentations :

- **Mod√®le** : `GradientBoostingRegressor`
- **r¬≤_test** : 0.707  
- **MAE** : 49.07  
- **MSE** : 4 445.72  

---

## üß∞ Stack technique

### Langage & Data

- Python
- NumPy
- pandas

### Machine Learning

- scikit-learn
- XGBoost
- (optionnel) LightGBM

### Orchestration & MLOps

- Apache Airflow
- **Astro CLI** (Astro Runtime)
- Docker

> üìù Astro Runtime inclut d√©j√† de nombreux *providers* Airflow pr√©‚Äëinstall√©s : voir la doc officielle  
> (section *Astro Runtime ‚Äì provider packages*).

### API & Front

- FastAPI
- Streamlit

### Autres biblioth√®ques

- matplotlib
- seaborn
- plotly
- joblib
- requests
- python-dotenv
- psycopg2-binary (PostgreSQL)
- boto3 / botocore / aiobotocore (int√©gration AWS S3, asynchrone si besoin)
- protobuf (compatibilit√© avec certains frameworks ML)

---

## üóÇÔ∏è Structure (simplifi√©e)

```text
.
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îú‚îÄ‚îÄ process_weather.py      # DAG d'entra√Ænement et pr√©paration des donn√©es
‚îÇ   ‚îî‚îÄ‚îÄ wheatheretl.py          # DAG ETL + inf√©rence / insertion en base
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ functions.py            # Fonctions utilitaires (ETL, features, mod√®les, S3, etc.)
‚îÇ   ‚îî‚îÄ‚îÄ inference.py            # Classe / fonctions d'inf√©rence
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îî‚îÄ‚îÄ main.py                 # App Streamlit ou FastAPI (selon organisation)
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ solar_radiation_predictions_ML.ipynb   # Notebook d'exploration / prototypage
‚îú‚îÄ‚îÄ Dockerfile                  # Image principale
‚îú‚îÄ‚îÄ docker-compose.yml          # Orchestration locale (Airflow, API, Streamlit, DB...)
‚îú‚îÄ‚îÄ requirements.txt            # D√©pendances Python
‚îî‚îÄ‚îÄ README.md                   # Ce fichier
```

> La structure exacte peut √©voluer, mais cette vue donne les grandes briques du projet.

---

## üîß Installation globale (sans Astro CLI)

### 1. Cloner le d√©p√¥t

```bash
git clone https://github.com/Thibaut-Longchamps/solar-radiation-forecasting.git
cd solar-radiation-forecasting
```

### 2. Cr√©er un environnement virtuel (optionnel mais recommand√©)

```bash
python -m venv .venv
source .venv/bin/activate      # macOS / Linux
# ou
.venv\Scripts\activate       # Windows
```

### 3. Installer les d√©pendances

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

---

## üöÄ Orchestration avec Airflow & Astro CLI

Cette partie d√©crit l‚Äôutilisation de **Apache Airflow** via **Astro CLI** pour orchestrer les DAGs de :

- pr√©paration & entra√Ænement des mod√®les (`process_weather.py`),
- ETL m√©t√©o + inf√©rence + insertion en base (`wheatheretl.py`).

### 1Ô∏è‚É£ Pr√©requis

- **Docker** install√© et fonctionnel.
- **Astro CLI** install√©.
- **AWS S3**

üìé Installation Astro CLI (r√©sum√©, voir doc Astronomer pour les d√©tails) :

- **macOS (Homebrew)**  
  ```bash
  brew install astro
  ```

- **Linux**  
  ```bash
  curl -sSL https://install.astronomer.io | sudo bash
  ```

- **Windows**  
  - Recommand√© : passer par **WSL2** et utiliser la commande Linux ci‚Äëdessus.

### 2Ô∏è‚É£ Initialisation (si projet Astro non encore cr√©√©)

Si ton repo n‚Äôest pas encore initialis√© en projet Astro :

```bash
astro dev init
```

Cela cr√©e la structure standard `.astro/`, les fichiers de base Airflow, etc.  
Tu peux ensuite d√©placer tes DAGs dans `dags/` et ton code dans `src/`.

### 3Ô∏è‚É£ D√©pendances Airflow (requirements)

Dans le projet Astro, assure-toi que `requirements.txt` contient au moins :

```txt
numpy==1.26.4
pandas
scikit-learn==1.5.2
xgboost==2.1.2
lightgbm
joblib==1.4.2
matplotlib==3.9.2
seaborn==0.13.2
plotly==5.24.1
boto3
botocore
aiobotocore
psycopg2-binary==2.9.10
python-dotenv==1.0.0
requests==2.32.3
protobuf==3.20.0
```

Astro Runtime se chargera d‚Äôinstaller ces paquets dans l‚Äôimage Airflow au d√©marrage.

### 4Ô∏è‚É£ Lancer l‚Äôenvironnement Airflow local

```bash
astro dev start
```

Cela va :

- builder l‚Äôimage Docker Airflow avec tes d√©pendances,
- lancer les containers Airflow (webserver, scheduler, DB, etc.).

Ensuite, tu peux acc√©der √† l‚ÄôUI Airflow :

- `http://localhost:8080`

### 5Ô∏è‚É£ Configurer les connexions Airflow

Dans l‚ÄôUI : **Admin ‚Üí Connections**.

#### Connexion PostgreSQL ‚Äì `postgres_default`

- Conn Id : `postgres_default`
- Conn Type : **Postgres**
- Host : selon ton `docker-compose` (ex : `postgres`)
- Port : `5432` ou mappage local
- Login / Password : ex. `postgres` / `postgres`
- Database : `postgres`

#### Connexion HTTP ‚Äì `open_meteo_api`

- Conn Id : `open_meteo_api`
- Conn Type : **HTTP**
- Host : `https://api.open-meteo.com/`

Utilis√©e pour r√©cup√©rer les donn√©es m√©t√©o (historiques ou temps r√©el).

#### Connexion AWS S3 ‚Äì `aws_s3_conn` (optionnel)

- Conn Id : `aws_s3_conn`
- Conn Type : **Amazon Web Services**
- Extra JSON, ex. :

```json
{
  "aws_access_key_id": "XXXXX",
  "aws_secret_access_key": "YYYYY",
  "region_name": "eu-west-3"
}
```

Utilis√©e pour stocker les mod√®les, pr√©dictions ou jeux de donn√©es sur S3.

### 6Ô∏è‚É£ Activer les DAGs

Depuis l‚Äôinterface Airflow :

- Activer le DAG d‚Äôentra√Ænement / pr√©paration (`process_weather.py`).
- Activer le DAG ETL m√©t√©o + inf√©rence (`wheatheretl.py`).

Surveiller l‚Äôex√©cution (onglets *Grid* / *Graph*).

---

## üß™ API FastAPI & Dashboard Streamlit

Selon l‚Äôorganisation du projet, tu peux avoir :

### üîπ API FastAPI (exemple)

D√©marrer l‚ÄôAPI :

```bash
uvicorn main:app --reload
```

- Endpoint exemple : `GET /predict` ou `POST /predict`
- Entr√©e : features m√©t√©o / temporelles
- Sortie : pr√©diction de GHI / production solaire.

### üîπ Streamlit

D√©marrer le dashboard :

```bash
streamlit run main.py
```

- Visualisation de la s√©rie temporelle de GHI
- Possibilit√© de comparer mod√®les, plages temporelles, etc.

Assure-toi que la cha√Æne de connexion PostgreSQL dans le code Streamlit est coh√©rente avec ton environnement (host, port, user, password).

---

## üê≥ Docker & docker-compose

### Build de l‚Äôimage

```bash
docker build -t solar-radiation-forecasting .
```

### D√©marrage via docker-compose

```bash
docker-compose up
```

En fonction de la configuration de `docker-compose.yml`, cela peut lancer :

- Airflow (webserver, scheduler, DB, etc.),
- l‚ÄôAPI FastAPI,
- Streamlit,
- PostgreSQL.

---

## ü§ù Contributions

Les contributions sont les bienvenues.

1. Forker le d√©p√¥t.
2. Cr√©er une branche de feature :  
   ```bash
   git checkout -b feature/ma-feature
   ```
3. Committer vos modifications :  
   ```bash
   git commit -m "Ajout nouvelle fonctionnalit√©"
   ```
4. Pousser la branche :  
   ```bash
   git push origin feature/ma-feature
   ```
5. Ouvrir une **Pull Request**.

---

## üìú Licence

Ce projet peut √™tre distribu√© sous une licence open-source (par exemple MIT).  
Adapter la section selon le fichier `LICENSE` pr√©sent dans le d√©p√¥t.
